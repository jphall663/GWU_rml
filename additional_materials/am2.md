### Lecture 2 Additional Software Tools

* **Python**:
  * [allennlp](https://github.com/allenai/allennlp)
  * [alibi](https://github.com/SeldonIO/alibi)
  * [anchor](https://oreil.ly/K3UuW)
  * [DiCE](https://oreil.ly/-lwV4)
  * [interpret](https://github.com/interpretml/interpret)
  * [lime](https://oreil.ly/j5Cqj)
  * [shap](https://github.com/slundberg/shap)
  * [PiML-Toolbox](https://github.com/SelfExplainML/PiML-Toolbox)
  * [tf-explain](https://github.com/sicara/tf-explain)

* **R**:
  * [ALEPlot](https://oreil.ly/OSfUT)
  * [DALEX](https://cran.r-project.org/web/packages/DALEX/index.html)
  * [ICEbox](https://oreil.ly/6nl1W)
  * [iml](https://cran.r-project.org/web/packages/iml/index.html)
  * [Model Oriented](https://oreil.ly/7wUMp)
  * [pdp](https://oreil.ly/PasMQ)
  * [shapFlex](https://oreil.ly/RADtC)
  * [vip](https://oreil.ly/YcD2_)

* **Python, R or other**:
  * [h2o-3](https://oreil.ly/GtGvK)

### Lecture 2 Additional Software Examples
  * [Global and Local Explanations of a Constrained Model](https://nbviewer.jupyter.org/github/jphall663/GWU_rml/blob/master/lecture_2.ipynb)
  * [Building from Penalized GLM to Monotonic GBM](https://nbviewer.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/glm_mgbm_gbm.ipynb?flush_cache=true)
  * [Monotonic XGBoost models, partial dependence, individual conditional expectation plots, and Shapley explanations](https://nbviewer.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/xgboost_pdp_ice.ipynb)
  * [Decision tree surrogates, LOCO, and ensembles of explanations](https://nbviewer.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/dt_surrogate_loco.ipynb)
  * _Machine Learning for High-risk Applications_: [Use Cases](https://oreil.ly/machine-learning-high-risk-apps-code) (Chapter 6)

### Lecture 2 Additional Reading

* **Introduction and Background**:
  * [*On the Art and Science of Explainable Machine Learning*](https://oreil.ly/myVr8)
  * [*Proposed Guidelines for the Responsible Use of Explainable Machine Learning*](https://arxiv.org/pdf/1906.03533.pdf)

* **Post-hoc Explanation Techniques**:
  * [_A Unified Approach to Interpreting Model Predictions_](https://papers.nips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf)
  * [_Anchors: High-Precision Model-Agnostic Explanations_](https://homes.cs.washington.edu/~marcotcr/aaai18.pdf)
  * [_Extracting Tree-Structured Representations of Trained Networks_](https://proceedings.neurips.cc/paper/1995/file/45f31d16b1058d586fc3be7207b58053-Paper.pdf)
  * [_Interpretability via Model Extraction_](https://arxiv.org/pdf/1706.09773.pdf)
  * **Interpretable Machine Learning** - [Chapter 6](https://christophm.github.io/interpretable-ml-book/agnostic.html) and [Chapter 7](https://christophm.github.io/interpretable-ml-book/example-based.html)
  * [_Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation_](https://arxiv.org/pdf/1309.6392.pdf)
  * [*Towards Better Understanding of Gradient-based Attribution Methods for Deep Neural Networks*](https://arxiv.org/pdf/1711.06104.pdf)
  * [_Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models_](https://arxiv.org/pdf/1612.08468.pdf)
  * [_“Why Should I Trust You?” Explaining the Predictions of Any Classifier_](https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf)

* **Problems with Post-hoc Explanation**:
  * [*General Pitfalls of Model-Agnostic Interpretation Methods*](https://oreil.ly/On9uS)
  * [_Limitations of Interpretable Machine Learning Methods_](https://oreil.ly/VHMWh)
  * [*When Not to Trust Your Explanations*](https://oreil.ly/9Oxa6)
