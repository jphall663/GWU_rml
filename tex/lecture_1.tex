% TODO: links in ecosystem models
% TODO: deeper XNN case study

\documentclass[11pt,aspectratio=169,hyperref={colorlinks}]{beamer}

\usetheme{Singapore}

\usecolortheme[snowy]{owl}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[american]{babel}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    urlcolor=[rgb]{0,0,0.61},
    linkcolor=[rgb]{0,0,0.61}}
\usepackage[natbib=true,style=authoryear,backend=bibtex,useprefix=true]{biblatex}
\usepackage{blindtext}

%-------------------------------------------------------------------------------

\usepackage{mathtools}
\usepackage{xcolor}
\usepackage{soul}
\newcommand{\mathcolorbox}[2]{\colorbox{#1}{$\displaystyle #2$}}

%-------------------------------------------------------------------------------

% OwlGreen - customized to make the header violet color
\definecolor{OwlGreen}{RGB}{51, 0, 102}

%-------------------------------------------------------------------------------

\setbeamertemplate{bibliography item}{}
\renewcommand*{\bibfont}{\scriptsize}
\addbibresource{lecture_1.bib}

\setbeamerfont{caption}{size=\footnotesize}
\setbeamertemplate{frametitle continuation}{}
\setcounter{tocdepth}{1}

%-------------------------------------------------------------------------------

\usenavigationsymbolstemplate{}
\setbeamertemplate{footline}{%
    \raisebox{5pt}{\makebox{\hfill\makebox[20pt]{\color{gray}
          \scriptsize\insertframenumber}}}\hspace*{5pt}}

\renewcommand*{\thefootnote}{\fnsymbol{footnote}}

%-------------------------------------------------------------------------------

\usepackage{epigraph}
% \epigraphsize{\small}% Default
\setlength\epigraphwidth{14cm}
\setlength\epigraphrule{0pt}
\usepackage{etoolbox}
\makeatletter
\patchcmd{\epigraph}{\@epitext{#1}}{\itshape\@epitext{#1}}{}{}
\makeatother

%-------------------------------------------------------------------------------

\author{Patrick Hall}
\title{Introduction to Responsible Machine Learning\footnote{\tiny{This material is shared under a \href{https://creativecommons.org/licenses/by/4.0/deed.ast}{CC By 4.0 license} which allows for editing and redistribution, even for commercial purposes. However, any derivative work should attribute the author.}}}
\subtitle{Lecture 1: Explainable Machine Learning Models}
\institute{The George Washington University}
\date{\today}

%-------------------------------------------------------------------------------

\begin{document}
	
	\maketitle
	
	\begin{frame}
	
		\frametitle{Contents}
		
		\tableofcontents{}
		
	\end{frame}
	

%-------------------------------------------------------------------------------
	\section{Class Overview}
%-------------------------------------------------------------------------------	
	\subsection*{}
	
	\begin{frame}
	
		\frametitle{Grading and Policy}
			
		\begin{itemize}
			\item{Grading:}
				\begin{itemize}
					\item{$\frac{6}{10}$ Weekly Assignments}
					\item{$\frac{3}{10}$ GitHub model card (\cite{model_cards})}
                    \item{$\frac{1}{10}$ Class participation}
				\end{itemize}
			\item{Project:}	
				\begin{itemize}
					\item{HMDA data using techniques from class}
					\item{Individual or group (no more than 4 members)}
					\item Groups randomly assigned by instructor, with consideration of time zone
				\end{itemize}
			\item{Syllabus}
			\item{Office hours: (??)}
			\item{Class resources: \url{https://jphall663.github.io/GWU_rml/}}	
		\end{itemize}		
			
	\end{frame}
	
	
	\begin{frame}
	
		\frametitle{Overview}
		
		\begin{itemize}
			\item{\textbf{Class 1}: Explainable Models}
			\item{\textbf{Class 2}: Post-hoc Explanations}
			\item{\textbf{Class 3}: Fairness}
			\item{\textbf{Class 4}: Security}
			\item{\textbf{Class 5}: Model Debugging}
			\item{\textbf{Class 6}: Best Practices}
		\end{itemize}
			
					
	\end{frame}

%-------------------------------------------------------------------------------
	\section{Introduction}
		\subsection*{} % for slide tracking
%-------------------------------------------------------------------------------
	
		\begin{frame}
	
			\frametitle{Responsible Artificial Intelligence}
	
			\epigraph{``The designing and building of intelligent systems that receive signals from the environment and take actions that affect that environment.''}{--- \citet{russell2010artificial}, \href{https://aima.cs.berkeley.edu/}{Artificial Intelligence: A Modern Approach}}

			\epigraph{``Responsible Artificial Intelligence is about human responsibility for the development of intelligent systems along fundamental human principles and values, to ensure human-flourishing and well-being in a sustainable world.''}{--- \citet{dignum2019responsible}, Responsible Artificial Intelligence} 
	
		\end{frame}		

		\begin{frame}
	
			\frametitle{Risk and Responsibility}
	
				\small{The \href{https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf}{NIST AI Risk Management Framework} (\citet{tabassi2023artificial}) characterizes risk as a ``composite measure of an eventâ€™s probability of occurring and the magnitude or degree of the consequences of the corresponding event. \textbf{The impacts, or consequences, of AI systems can be positive, negative, or both and can result in opportunities or threats.}''}\\
				\vspace{10pt}
				\small{The \href{https://artificialintelligenceact.eu/}{Draft European Union AI Act} categorizes the following ML applications as high risk: biometric identification; management of critical infrastructure; education; employment; essential services, both public (e.g., public assistance) and private (e.g., credit lending); law enforcement; immigration and border control; criminal justice; and the democratic process.}\\
				\vspace{10pt}
				\small{Increased financial, legal, regulatory, or ethical considerations in high-risk applications should inspire practitioners to act with greater responsibility.}

		\end{frame}	

		\begin{frame}

			\frametitle{What About Machine Learning?}

			\begin{columns}
		
				\column{0.5\linewidth}
					\textit{``[A] field of study that gives computers the ability to learn without being explicitly programmed.''\\ \hspace{20pt} --- \textup{Arthur Samuel}, circa 1960}
	
				\column{0.5\linewidth}
						\centering
						\includegraphics[height=150pt]{../img/cs_stat.png}
						\label{fig:cs_stat}

			\end{columns}
	
		\end{frame}	

		\begin{frame}
		
			\frametitle{A Responsible Machine Learning Workflow}
			
			\begin{figure}[htb]
				\begin{center}
					\includegraphics[height=150pt]{../img/rml_diagram_no_hilite.png}\\\vspace{5pt}
					\label{fig:blueprint_nohl}
					\scriptsize{\textbf{Source:} \href{https://www.mdpi.com/2078-2489/11/3/137/htm}{\textit{A Responsible Machine Learning Workflow}}. (\citet{rml_workflow})}
				\end{center}
			\end{figure}
		
		\end{frame}
		
		\begin{frame}
	
			\frametitle{A Responsible ML Workflow: Explainable Models}		
			
			\begin{figure}[htb]
				\begin{center}
					\includegraphics[height=150pt]{../img/rml_diagram_lec1_hilite.png}\\\vspace{5pt}
					\label{fig:blueprint_l1hl}
					\scriptsize{\textbf{Source:} \href{https://www.mdpi.com/2078-2489/11/3/137/htm}{\textit{A Responsible Machine Learning Workflow}}. (\citet{rml_workflow})}
				\end{center}
			\end{figure}		
					
		\end{frame}					

		\begin{frame}
	
			\frametitle{Explainable ML Models}			
					
			\textbf{Interpretation}: a high-level, meaningful mental representation that contextualizes a stimulus and leverages human background knowledge. An interpretable model should provide users with a description of what a data point or model output means \textit{in context} (\cite{broniatowski2021psychological}).\\
			
			\vspace{10pt}
			
			\textbf{Explanation}: a low-level, detailed mental representation that seeks to describe some complex process. An ML explanation is a description of how some model mechanism or output \textit{came to be} (\cite{broniatowski2021psychological}).
			
		\end{frame}
			
		\begin{frame}
	
			\frametitle{Explainable ML Models}			
	
			\small
			
			There are many types of explainable ML models. Some might be directly interpretable to non-technical consumers. Some are only explainable to highly-skilled data scientists. Interpretability is not an on-and-off switch.
			
			\vspace{10pt}
			
			Explainable models are crucial for risk management, documentation, compliance, explanation of predictions to consumers, finding and fixing discrimination, and  debugging other problems in ML modeling pipelines. Simply put, \textbf{it is very difficult to mitigate risks you don't understand}.
			
			\vspace{10pt}
			
			There is not necessarily a trade-off between accuracy and explainabillity, especially for structured data.
			
			\normalsize
			
		\end{frame}	

		\begin{frame}
	
			\frametitle{Some Characteristics of Explainable ML Models\\ (\small{\cite{sudjianto2021designing})}}			
			
			\small
			
			\begin{itemize}
				\item \textbf{Additivity}: Whether/how model takes an additive or modular form. Additive decomposition of feature effects tends to be more explainable.
				\item \textbf{Sparsity}: Whether/how features or model components are regularized. Having fewer features or components tends to be more explainable.
				\item \textbf{Linearity}: Whether/how feature effects are linear. Linear or constant feature effects are easy to explain.
				\item \textbf{Smoothness}: Whether/how feature effects are continuous and smooth. Continuous and smooth feature effects are relatively easy to explain.
				\item \textbf{Monotonicity}: Whether/how feature effects can be modeled to be monotone. When increasing/decreasing effects are desired by expert knowledge they are easy to explain.
				\item \textbf{Visualizability}: Whether/how the feature effects can be directly visualized. Visualization facilitates the final model diagnostics and explanation.
			\end{itemize}
			
			\normalsize
			
		\end{frame}	

		\begin{frame}
		
			\frametitle{Background}		
			
			We will frequently refer to the following terms and definitions today: \\			
			
			\begin{itemize}
				\item{\textbf{Pearson correlation}: Measurement of the linear relationship between two input $X_j$ features; takes on values between -1 and +1, including 0.}
				\item{\textbf{Shapley value}: a quantity, based in game theory, that accurately decomposes the outcomes of complex systems, like ML models, into individual components.}
				\item{\textbf{Partial dependence and individual conditional expectation (ICE)}: Visualizations of the behavior of $X_j$ under some model $g$.}
			\end{itemize}			
		
		\end{frame}

		\begin{frame}[allowframebreaks]
	
			\frametitle{Background: Notation}
			
			\textbf{Spaces} 
			
			\begin{itemize}
				\item Input features come from the set $\mathcal{X}$ contained in a \textit{P}-dimensional input space, $\mathcal{X} \subset \mathbb{R}^P$.  An arbitrary, potentially unobserved, or future instance of $\mathcal{X}$ is denoted $\mathbf{x}$, $\mathbf{x} \in \mathcal{X}$.
				\item Labels corresponding to instances of $\mathcal{X}$ come from the set $\mathcal{Y}$.
				\item Learned output responses come from the set $\mathcal{\hat{Y}}$.
			\end{itemize}	
			
			\framebreak	
			
			\textbf{Datasets} 
			
			\begin{itemize}
				\item The input dataset $\mathbf{X}$ is composed of observed instances of the set $\mathcal{X}$ with a corresponding dataset of labels $\mathbf{Y}$, observed instances of the set $\mathcal{Y}$. 
				\item Each $i$-th observation of $\mathbf{X}$ is denoted as\\ $\mathbf{x}^{(i)} = $  
				$[x_0^{(i)}, x_1^{(i)}, \dots, x_{\textit{P}-1}^{(i)}]$, with corresponding $i$-th labels in $\mathbf{Y}, \mathbf{y}^{(i)}$, and corresponding predictions in $\mathbf{\hat{Y}}, \mathbf{\hat{y}}^{(i)}$.
				\item $\mathbf{X}$ and $\mathbf{Y}$ consist of $N$ tuples of observations:\\ $[(\mathbf{x}^{(0)},\mathbf{y}^{(0)}), (\mathbf{x}^{(1)},\mathbf{y}^{(1)}), \dots,(\mathbf{x}^{(N-1)},\mathbf{y}^{(N-1)})]$.
				\item Each $j$-th input column vector of $\mathbf{X}$ is denoted as $X_j = [x_{j}^{(0)}, x_{j}^{(1)}, \dots, x_{j}^{(N-1)}]^T$.
			\end{itemize}	 
			
			\framebreak
			
			\textbf{Models}
			
			\begin{itemize}
				\item A type of machine learning (ML) model $g$, selected from a hypothesis set $\mathcal{H}$, is trained to represent an unknown signal-generating function $f$ observed as  $\mathbf{X}$ with labels $\mathbf{Y}$ using a training algorithm $\mathcal{A}$: 
				$ \mathbf{X}, \mathbf{Y} \xrightarrow{\mathcal{A}} g$, such that $g \approx f$.
				\item $g$ generates learned output responses on the input dataset $g(\mathbf{X}) = \mathbf{\hat{Y}}$, and on the general input space $g(\mathcal{X}) = \mathcal{\hat{Y}}$.
				\item The model to be explained, tested for discrimination, or debugged is denoted as $g$.
			\end{itemize}
			
		\end{frame}
	
		\begin{frame}
		
			\frametitle{Background: Gradient Boosting Machine}			
			
			\begin{equation}
			\begin{aligned}\label{eq:gbm}
			g^{\text{GBM}}(\mathbf{x}) &= \sum_{b=0}^{B-1} T_b\left(\mathbf{x}; \Theta\right)
			\end{aligned}
			\end{equation}
		
			\vspace{20pt}
		
			A GBM is a sequential combination of decision trees, $T_b$, where $T_0$ is trained to predict $\mathbf{y}$, but all subsequent $T$ are trained to reduce the errors of $T_{b-1}$.
		
		\end{frame}	

		%\begin{frame}
		%
		%	\frametitle{Background: Shapley Value}	
		%	
		%	Shapley explanations, including TreeSHAP and even certain implementations of LIME, are a class of additive, locally accurate feature contribution measures with long-standing theoretical support (\cite{shapley}). 
		%
		%	\vspace{8pt}
		%	
		%	For some observation $\mathbf{x} \in \mathcal{X}$, Shapley explanations take the form:
		%	
		%	\begin{equation}
		%		\label{eq:shap_contrib}
		%		\begin{aligned}
		%			\phi_{j} = \underbrace{\sum_{S \subseteq \mathcal{P} \setminus \{j\}}\frac{|S|!(\mathcal{P} -|S| -1)!}{\mathcal{P}!}}_\text{weighted average over all subsets in \textbf{X}}\underbrace{[(S \cup \{j\}) - g_x(S)]}_{g\text{ "without" }x_j}
		%		\end{aligned}
		%	\end{equation}
		%	
		%	\begin{equation}
		%		\label{eq:shap_additive}
		%		\begin{aligned}
		%			g(\mathbf{x}) = \phi_0 + \sum_{j=0}^{j=\mathcal{P} - 1} \phi_j \mathbf{z}_j
		%		\end{aligned}
		%	\end{equation}
			
		%\end{frame}
		
		%\begin{frame}
		%
		%	\frametitle{Background: Partial Dependence and ICE}			
		%
		%	\begin{itemize}
		%
		%	\item Following \citet{esl} a single input feature, $X_j \in \mathbf{X}$, and its complement set, $\mathbf{X}_{\mathcal{P} \setminus \{j\}} \in \mathbf{X}$, where $X_j \cup \mathbf{X}_{\mathcal{P} \setminus \{j\}} = \mathbf{X}$ is considered. $\text{PD}(X_j, g)$ for a given feature $X_j$ is estimated as the average output of the learned function $g(\mathbf{X})$ when all the components of $X_j$ are set to a constant $x \in \mathcal{X}$ and $\mathbf{X}_{(-j)}$ is left unchanged.
		%
		%	\item $\text{ICE}(x_j, \mathbf{x}, g)$ for a given instance $\mathbf{x}$ and feature $x_j$ is estimated as the output of $g(\mathbf{x})$ when $x_j$ is set to a constant $x \in \mathcal{X}$ and all other features $\mathbf{x} \in \mathbf{X}_{(-j)}$ are left untouched. Partial dependence and ICE curves are usually plotted over some set of constants $x \in \mathcal{X}$ (\cite{ice_plots}). 
		%
		%	\end{itemize} 			
		%			
		%\end{frame}		
							
%-------------------------------------------------------------------------------
	\section{The GAM Family}
		\subsection*{}
%-------------------------------------------------------------------------------

		\begin{frame}
	
		\frametitle{The GAM Family of Explainable Models (fANOVA)}	
				
			\begin{flalign}
			&\begin{aligned}\label{eq:glm1}
			g^{\text{GLM}}(\mathbf{x}) &= \beta_0 + \beta_1 x_0 + \beta_2 x_1 + \dots + \beta_P x_{P-1}
			\end{aligned}&&
			\end{flalign}
			
			\begin{flalign}
			&\begin{aligned}\label{eq:gam1}
			g^{\text{GAM}}(\mathbf{x}) &= \beta_0 + \beta_1 g_0(x_0) + \beta_2 g_1(x_1) + \dots + \beta_P g_{P-1}(x_{P-1})
			\end{aligned}&&
			\end{flalign}
	
			\begin{flalign}
			&\begin{aligned}\label{eq:ga2m1}
			g^{\text{GA2M}}(\mathbf{x}) = \beta_0 + \beta_1 g_0(x_0) + \beta_2 g_1(x_1) + \dots + \beta_P g_{P-1}(x_{P-1}) + \dots +\\
 			\beta_{0,1} g_{0,1}(x_0, x_1) + \dots + \beta_{P-2,P-1} g_{P-2,P-1}(x_{P-2}, x_{P-1})
			\end{aligned}&&
			\end{flalign}

		
		\vspace{10pt} 
		\noindent \small{Where shape functions are fit with traditional spline techniques in GAM and GA2M, and shape functions are fit with boosting and neural networks in GA2M variants like explainable boosting machines (EBMs) and neural additive models (NAMs), respectively}.
		
		\end{frame}

		\begin{frame}
	
		\frametitle{Anatomy of Elastic Net Regression}	
				
		Penalized linear models have the same basic functional form as more traditional linear models, e.g. ...
				
		\begin{equation}
			\begin{aligned}\label{eq:glm2}
			g^{\text{GLM}}(\mathbf{x}) &= \beta_0 + \beta_1 x_0 + \beta_2 x_1 + \dots + \beta_P x_{P-1}
			\end{aligned}
		\end{equation}	
		
		\vspace{10pt}... but are more robust to correlation, wide data, and outliers.
		
		\end{frame}

		\begin{frame}
		
			\frametitle{Anatomy of Elastic Net Regression: L1 and L2 Penalty}			
		Iteratively reweighted least squares (IRLS) method with ridge ($L_2$) and LASSO ($L_1$) penalty terms: 
			
			\begin{equation}
				\label{eq:Elastic_Net}
				\begin{aligned}
				\tilde{\beta}= \underset{\beta}{min}\Big\{ \mathcolorbox{red}{ \underbrace{\sum_{i=0}^{N-1}(y_i-\beta_0-\sum_{j=1}^{P-1} x_{ij} \beta_{j})^2}_\text{1}} + \mathcolorbox{red}{\underbrace{\lambda}_\text{2}} \sum_{j=1}^{P-1} ( \mathcolorbox{red}{\underbrace{\alpha}_\text{3}} \mathcolorbox{red}{\underbrace{\beta_j^2}_\text{4}} + (1-\mathcolorbox{red}{\underbrace{\alpha}_\text{3}}) \mathcolorbox{red}{\underbrace{|\beta_{j}|}_\text{5}}) \Big\}
				\end{aligned}
			\end{equation}		
			
			\begin{itemize}
			\scriptsize{
				\item{1: Least squares minimization}
				\item{2: Controls magnitude of penalties}
				\item{3: Tunes balance between L1 and L2}
				\item{4: $L_2$/Ridge penalty term}
				\item{5: $L_1$/LASSO penalty term}}
			\end{itemize}
						
		\end{frame}
	
		\begin{frame}
			
			\textbf{Graphical Illustration of Shrinkage/Regularization Method:} 
			
			\begin{figure}[htb]
				\begin{center}
					\includegraphics[height=150pt]{../img/L1L2_penalty_diagram.png}
					\label{fig:L1L2}
				\end{center}
			\end{figure}
								
		\end{frame}				

	\begin{frame}
		
		\frametitle{Generalized Additive Models and Explainable Boosting Machines}
		
		\small
		
		Generalized additive models (GAMs, \cite{esl}) extend GLMs by allowing an arbitrary function for each $X_j$: 	
		
		\begin{equation}
			\begin{aligned}\label{eq:gam}
			g^{\text{GAM}}(\mathbf{x}) &= \beta_0 + \beta_1 g_0(x_0) + \beta_2 g_1(x_1) + \dots + \beta_P g_{P-1}(x_{P-1})
			\end{aligned}
		\end{equation}	
		
		GAMs use spline approaches to fit each $g_j$.\\
		\vspace{10pt}
		Later \cite{ga2m} introduced an efficient technique for finding interaction terms ($\beta_{j,k} g_{j,k}(x_j, x_k)$) to include in GAMs. This highly accurate technique was given the acronym GA2M.\\
		\vspace{10pt}
		Recently Microsoft Research introduced the explainable boosting machine (EBM) in the \href{https://github.com/interpretml/interpret/}{interpret} package, in which GBMs are used to fit each $g_{j}$ and $g_{j,k}$. Higher order interactions are allowed, but used infrequently in practice. \\
		\vspace{10pt}
		Because each input feature, or combination thereof, is treated separately and in an additive fashion, explainability is very high. 
		
	\end{frame}

	\begin{frame}
	
		\frametitle{Generalized Additive Models and Explainable Boosting Machines}
		
		\begin{figure}[htb]
			\begin{center}
				\includegraphics[height=190pt]{../img/ebm.png}
				\label{fig:ebm}
			\end{center}
		\end{figure}
	
	\end{frame}

	\begin{frame}
	
		\frametitle{Generalized Additive Models and Neural Networks}
		
		\noindent Researchers have also put forward GA2M variants in which each $g_{j}$ and $g_{j, k}$ shape function is fit by neural networks, e.g., GAMI-Net (\citet{yang2021gami}) and neural additive models (\citet{agarwal2021neural}).\\
		\vspace{10pt} 
		\noindent See the \href{https://selfexplainml.github.io/PiML-Toolbox/_build/html/index.html}{PiML package} for an excellent implementation of GAMI-Net and other explainable models. 
		 
	\end{frame}


%-------------------------------------------------------------------------------
	\section{Monotonic GBM}
%-------------------------------------------------------------------------------

	\subsection*{}
	
	\begin{frame}
	
		\frametitle{Monotonic GBM (\cite{rml_workflow})}

			Monotonic GBM (MGBM) constrain typical GBM training to consider only tree splits that obey user-defined positive and negative monotone constraints, with respect to each input feature, $X_j$, and a target feature, $\mathbf{y}$, independently. An MGBM remains an additive combination of $B$ trees trained by gradient boosting, $T_b$, and each tree learns a set of splitting rules that respect monotone constraints,  $\Theta^\text{mono}_b$. A trained MGBM model, $g^{\text{MGBM}}$, takes the form:
			
			\begin{equation}
			\begin{aligned}\label{eq:gbm}
			g^{\text{MGBM}}(\mathbf{x}) &= \sum_{b=0}^{B-1} T_b\left(\mathbf{x}; \Theta^\text{mono}_b\right)
			\end{aligned}
			\end{equation}
		
	\end{frame}


	\begin{frame}
	
	\frametitle{Monotone Constraints for GBM (\cite{rml_workflow})}
	
		\begin{enumerate}\scriptsize
			\item For the first and highest split in $T_b$ involving $X_j$, any $\theta_{b,j,0}$ resulting in $T(x_j; \theta_{b,j,0}) = \{w_{b,j,0,L}, w_{b,j,0,R}\}$ where $w_{b,j,0,L} > w_{b,j,0,R}$, is not considered. 
			\item For any subsequent left child node involving $X_j$, any $\theta_{b,j, k\ge1}$ resulting in $T(x_j; \theta_{b,j,k\ge1}) = \{w_{b,j,k\ge1,L}, w_{b,j,k\ge1,R}\}$ where $w_{b,j,k\ge1,L} > w_{b,j,k\ge1,R}$, is not considered.
			\item Moreover, for any subsequent left child node involving $X_j$, $T(x_j; \theta_{b,j,k\ge1}) = \{w_{b,j,k\ge1,L}, w_{b,j,k\ge1,R}\}$, $\{w_{b,j,k\ge1,L}, w_{b,j,k\ge1,R}\}$ are bound by the associated $\theta_{b,j,k-1}$ set of node weights, $\{w_{b,j,k-1,L}, w_{b,j,k-1, R}\}$, such that $ \{w_{b,j,k\ge1,L}, w_{b,j,k\ge1,R}\} < \frac{w_{b,j,k-1,L} + w_{b,j,k-1,R}}{2}$.
			\item (1) and (2) are also applied to all right child nodes, except that for right child nodes $ w_{b,j,k,L} \le w_{b,j,k,R}$ and $\{w_{b,j,k\ge1,L}, w_{b,j,k\ge1,R}\} \ge \frac{w_{b,j,k-1,L} + w_{b,j,k-1,R}}{2}$.
		\end{enumerate}
	
	Note that $g^{\text{MGBM}}(\mathbf{x})$ is an addition of each full $T_b$ prediction, with the application of a monotonic logit or softmax link function for classification problems. Moreover, each tree's root node corresponds to some constant node weight that by definition obeys monotonicity constraints, $ T(x^{\alpha}_j; \theta_{b,0}) = T(x^{\beta}_j; \theta_{b,0}) = w_{b,0}$. 
	
	\end{frame}

	\begin{frame}
	
			\textbf{Partial Dependence and ICE:}

			\begin{figure}[htb]
				\begin{center}
					\includegraphics[height=150pt]{../img/mort_mgbm_glob_pdp_ice.png}
					\label{fig:mgbm}
				\end{center}
			\end{figure}
	
	\end{frame}

%-------------------------------------------------------------------------------
	\section{An Ecosystem}
%--------------------------------------------------------------------------
			
		\subsection*{}
		
		\begin{frame}
		
			\frametitle{A Burgeoning Ecosystem of Explainable Machine Learning Models}		
			
			\begin{itemize}
				\item \href{https://www.mdpi.com/2078-2489/11/3/137}{Explainable Neural Network} (XNN) (\cite{wf_xnn})
				\item Rudin group: 
				\begin{itemize}
					\item \href{https://www.youtube.com/watch?v=k3IQnRsl9U4}{\textit{This looks like that} deep learning} (\cite{this_looks_like_that})
					\item Scalable Bayesian rule list (\cite{sbrl}) 
					\item Optimal sparse decision tree (\cite{osdt})
					\item Supersparse linear integer models (\cite{slim})
					\item and more ... 
				\end{itemize}
				\item \href{https://github.com/scikit-learn-contrib/skope-rules}{rpart}
				\item \href{https://christophm.github.io/interpretable-ml-book/rulefit.html}{RuleFit} (\cite{rulefit})
				\item \href{https://github.com/scikit-learn-contrib/skope-rules}{skope rules}
			\end{itemize}
		
		\end{frame}

%--------------------------------------------------------------------------
	\section{Model Selection}
%--------------------------------------------------------------------------

	\subsection*{}
	
	\begin{frame}
	
		\frametitle{Model Selection}		
	
	\begin{columns}
		
		\column{0.5\linewidth}
		\begin{itemize}
			\item Generally speaking, standard ML evaluation -- including Kaggle leaderboards, are poor ways to assess ML model performance.
			\item However, \cite{caruana2004kdd} puts forward a robust model evaluation and selection technique based on cross-validation and ranking. 
			\item PiML contains excellent real-world model validation approaches as well.
		\end{itemize}
	
		\column{0.6\linewidth}
		\centering
		\includegraphics[height=130pt]{../img/cv_ranking.png}\vspace{5pt}
		\scriptsize{Three models are ranked across different metrics and folds. The model with the highest rank, on average, across metrics and folds is the best model, \texttt{gbm11} in this case.}
		
	\end{columns}
	
\end{frame}	

%--------------------------------------------------------------------------
\section{Acknowledgments}
%--------------------------------------------------------------------------

\subsection*{}

\begin{frame}[t]
	
	\frametitle{Acknowledgments}		
	
	Thanks to Lisa Song for her assistance in developing these course materials.\\
	
\end{frame}
		
%-------------------------------------------------------------------------------
%	References
%-------------------------------------------------------------------------------

	\begin{frame}[t, allowframebreaks]
	
		\frametitle{References}	
		
		\printbibliography
		
	\end{frame}

\end{document}
