\documentclass[11pt,aspectratio=169,hyperref={colorlinks}]{beamer}

\usetheme{Singapore}

\usecolortheme[snowy]{owl}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[american]{babel}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    urlcolor=[rgb]{0,0,0.61},
    linkcolor=[rgb]{0,0,0.61}}
 
%-------------------------------------------------------------------------------
    
\usepackage[natbib=true,style=numeric,backend=bibtex,useprefix=true]{biblatex}
\setbeamertemplate{bibliography item}{\insertbiblabel}
\setbeamerfont{caption}{size=\footnotesize}
\setbeamertemplate{frametitle continuation}{}
\renewcommand*{\bibfont}{\scriptsize}
\addbibresource{lecture_2.bib}

%-------------------------------------------------------------------------------

\setcounter{tocdepth}{1}

%-------------------------------------------------------------------------------

\renewcommand*{\thefootnote}{\fnsymbol{footnote}}
\usenavigationsymbolstemplate{}
\setbeamertemplate{footline}{%
    \raisebox{5pt}{\makebox{\hfill\makebox[20pt]{\color{gray}
          \scriptsize\insertframenumber}}}\hspace*{5pt}}

%-------------------------------------------------------------------------------

\usepackage{epigraph}
\setlength\epigraphwidth{14cm}
\setlength\epigraphrule{0pt}
\usepackage{etoolbox}
\makeatletter
\patchcmd{\epigraph}{\@epitext{#1}}{\itshape\@epitext{#1}}{}{}
\makeatother

%-------------------------------------------------------------------------------

\usepackage{mathtools}
\usepackage{xcolor}
\usepackage{soul}
\newcommand{\mathcolorbox}[2]{\colorbox{#1}{$\displaystyle #2$}}

%-------------------------------------------------------------------------------

% OwlGreen - customized to make the header violet color
\definecolor{OwlGreen}{RGB}{51,0,102}
\definecolor{magenta}{RGB}{248,206,204}
\definecolor{fuschia}{RGB}{255,0,255}
\definecolor{blue}{RGB}{0,0,156}
\usepackage{xcolor, soul}
\sethlcolor{magenta}

%-------------------------------------------------------------------------------

\setbeamertemplate{bibliography item}{}
\renewcommand*{\bibfont}{\scriptsize}

\setbeamerfont{caption}{size=\footnotesize}
\setbeamertemplate{frametitle continuation}{}
\setcounter{tocdepth}{1}

%-------------------------------------------------------------------------------

\usenavigationsymbolstemplate{}
\setbeamertemplate{footline}{%
    \raisebox{5pt}{\makebox{\hfill\makebox[20pt]{\color{gray}
          \scriptsize\insertframenumber}}}\hspace*{5pt}}

\renewcommand*{\thefootnote}{\fnsymbol{footnote}}

%-------------------------------------------------------------------------------

\author{Patrick Hall}
\title{Responsible Machine Learning\footnote{\tiny{This material is shared under a \href{https://creativecommons.org/licenses/by/4.0/deed.ast}{CC By 4.0 license} which allows for editing and redistribution, even for commercial purposes. However, any derivative work should attribute the author.}}}
\subtitle{Lecture 2:  Post-hoc Explanations}
\institute{The George Washington University}
\date{\today}



\begin{document}
	
	\maketitle
	
	\begin{frame}
	
		\frametitle{Contents}
		
		\tableofcontents{}
		
	\end{frame}

\renewcommand*{\thefootnote}{\arabic{footnote}}

%-------------------------------------------------------------------------------
	\section{Introduction}
%-------------------------------------------------------------------------------

	
		\subsection*{} %just for progress indicator
			
		\begin{frame}
		
			\frametitle{A Responsible Machine Learning Workflow\footnote{\href{https://www.mdpi.com/2078-2489/11/3/137/htm}{\textit{A Responsible Machine Learning Workflow}}}}
			
			\begin{figure}[htb]
				\begin{center}
					\includegraphics[height=150pt]{../img/rml_diagram_lec2_hilite.png}
					\label{fig:blueprint}
				\end{center}
			\end{figure}		
					
		\end{frame}					

	\begin{frame}[t]
		
		\frametitle{What is an Explanation in Machine Learning (ML)?}
		
		\epigraph{“An explanation of a model result is a description of how a model’s outcome came to be.”}{--- \textup{David Broniatowski}, GWU\\\href{https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=931426}{Psychological {F}oundations of {E}xplainability and {I}nterpretability in {A}rtificial {I}ntelligence} \cite{broniatowski2021psychological}}		
		
		\scriptsize Variously defined along with aliases or similar concepts:
		\begin{itemize}\scriptsize
			\item ``Towards a Rigorous Science of Interpretable Machine Learning'' (\citet{been_kim1})
			\item ``Explaining Explanations'' (\citet{gilpin2018explaining})
			\item ``A Survey Of Methods For Explaining Black Box Models'' (\citet{guidotti2018survey})
			\item ``The Mythos of Model Interpretability'' (\citet{lipton1})
		 	\item \textit{Interpretable Machine Learning} (\citet{molnar})
			\item ``Interpretable Machine Learning: Definitions, Methods, and Applications'' (\citet{murdoch2019interpretable})
			\item ``Challenges for Transparency'' (\citet{weller2017challenges}). 
		\end{itemize}\normalsize
		
	\end{frame}
	
	\begin{frame}
	
		\frametitle{What do \textit{\textbf{I}} Mean by Explainable ML?}

		Mostly post-hoc techniques used to enhance \textit{\textbf{understanding}} of trained model mechansims and predictions, e.g. ...
		\begin{itemize}
			\item \textbf{Direct measures of global and local feature importance}: 
			\begin{itemize}\footnotesize
				\item Gradient-based feature attribution (\citet{grad_attr})
				\item Shapley values (\citet{shapley}, \citet{shapley1953value})
			\end{itemize}
			\item \textbf{Global and local surrogate models}: 
			\begin{itemize}\footnotesize
				\item Decision tree variants (\citet{viper}, \citet{dt_surrogate1})
				\item Anchors (\citet{anchors})
				\item Local interpretable model-agnostic explanations (LIME) (\citet{lime})
			\end{itemize}
			\item \textbf{Global and local visualizations of trained model predictions}: 
			\begin{itemize}\footnotesize
				\item Accumulated local effects (ALE) (\citet{ale_plot}) 
				\item Partial dependence (\citet{esl})
				\item Individual conditional expectation (ICE) (\citet{ice_plots})
			\end{itemize}
			\item \textbf{Counterfactual explanations} (\citet{mothilal2020explaining})
			\item \textbf{Example-based explanations} (\citet{molnar})
		\end{itemize}\normalsize
			
	\end{frame}

	%-------------------------------------------------------------------------------
	\section{Techniques}
	%-------------------------------------------------------------------------------
	
	\subsection*{} % just for progress indicator


	\begin{frame}
	
		\frametitle{Shapley Value}	
	
		Shapley explanations, including TreeSHAP and even certain implementations of LIME, are a class of additive, locally accurate feature contribution measures with long-standing theoretical support (\cite{shapley}). 
	
		\vspace{8pt}
	
		For some observation $\mathbf{x} \in \mathcal{X}$, Shapley explanations take the form:
	
		\begin{equation}
		\label{eq:shap_contrib}
		\begin{aligned}
		\phi_{j} = \underbrace{\sum_{S \subseteq \mathcal{P} \setminus \{j\}}\frac{|S|!(\mathcal{P} -|S| -1)!}{\mathcal{P}!}}_\text{weighted average over all subsets in \textbf{X}}\underbrace{[g_x(S \cup \{j\}) - g_x(S)]}_{g\text{ "without" }x_j}
		\end{aligned}
		\end{equation}
		
		\begin{equation}
		\label{eq:shap_additive}
		\begin{aligned}
		g(\mathbf{x}) = \phi_0 + \sum_{j=0}^{j=\mathcal{P} - 1} \phi_j \mathbf{z}_j
		\end{aligned}
		\end{equation}
	
	\end{frame}

	\begin{frame}[t]
	
		\frametitle{Surrogate Decision Trees (DT)}
		
		\begin{figure}[htb]
			\begin{center}
				\includegraphics[height=90pt]{../img/dt_surrogate.png}
				\label{fig:dt_surrogate}
				\caption{$h_{\text{tree}}$ for Taiwanese credit card data \cite{uci}, and for machine-learned GBM response function $g$.}
			\end{center}
		\end{figure}
		
		\vspace{-20 pt}
		
		\begin{itemize}
			
			\item Given a learned function $g$ and set of predictions $g(\mathbf{X})$, a surrogate DT can be trained: $ \mathbf{X}, g(\mathbf{X}) \xrightarrow{\mathcal{A}_{\text{surrogate}}} h_{\text{tree}}$.
			
			\item $h_{\text{tree}}$ displays a low-fidelity, high-interpretability flow chart of $g$'s decision making process, and important features and interactions in $g$.	
			
		\end{itemize}
		
	\end{frame}
	
	\begin{frame}
		
		\frametitle{Surrogate Decision Trees (DT)}
		
		\begin{itemize}
			
			\item Always use error measures to assess the trustworthiness of $h_{\text{tree}}$.
			
			\item Prescribed methods (\cite{dt_surrogate1}; \cite{dt_surrogate2}) for training $h_{\text{tree}}$ do exist. In practice, straightforward cross-validation approaches are typically sufficient. 
			
			\item Comparing cross-validated training error to traditional training error can give an indication of the stability of the single tree model, $h_{\text{tree}}$.
			
			\item Hu et al. (\citeyear{lime-sup}) use local linear surrogate models, $h_{\text{GLM}}$, in $h_{\text{tree}}$ leaf nodes to increase overall surrogate model fidelity while also retaining a high degree of interpretability.
			
		\end{itemize}
	
	\end{frame}

	\begin{frame}

		\frametitle{Local Interpretable Model-agnostic Explanations (LIME)}
		
		Ribeiro, Singh, and Guestrin (\citeyear{lime}) define LIME for some observation $\mathbf{x} \in \mathcal{X}$:
		
		\begin{equation*}
		\begin{aligned}
		\underset{h \in \mathcal{H}}{\arg\min}\:\mathcal{L}(g, h, \pi_{\mathbf{X}}) + \Omega(h)
		\end{aligned}
		\end{equation*}
		
		Here $g$ is the function to be explained, $h$ is an interpretable surrogate model of $g$, often a linear model $h_{GLM}$, $\pi_{\mathbf{X}}$ is a weighting function over the domain of $g$, and $\Omega(h)$ limits the complexity of $h$.
		
		\vspace{5pt}
		
		Typically, $h_{GLM}$ is constructed such that $\mathbf{X}^{(*)}, g({X}^{(*)}) \xrightarrow{\mathcal{A}_{\text{surrogate}}} h_{\text{GLM}}$, where $\mathbf{X}^{(*)}$ is a generated sample, $\pi_{\mathbf{X}}$ weighs $\mathbf{X}^{(*)}$ samples by their Euclidean similarity to $\mathbf{x}$, local feature importance is estimated using $\beta_j x_j$, and $L_1$ regularization is used to induce a simplified, sparse $h_{GLM}$. 		


	\end{frame}

	\begin{frame}
		
		\frametitle{Local Interpretable Model-agnostic Explanations (LIME)}
		
		\begin{itemize}
			
			\item LIME is ideal for creating low-fidelity, highly interpretable explanations for non-DT models and for neural network models trained on unstructured data, e.g. deep learning.
			
			\item Always use regression fit measures to assess the trustworthiness of LIMEs.
			
			\item LIME can be difficult to deploy, but there are highly deployable variants. (\cite{lime-sup}; \cite{h2o_mli_booklet})
			
			\item Local feature importance values are offsets from a local intercept.
			
			\begin{itemize}
				
				\item Note that the intercept in LIME can account for the most important local phenomena.
				
				\item Generated LIME samples can contain large proportions of out-of-range data that can lead to unrealistic intercept values. 
				
			\end{itemize}
			
		\end{itemize}
		
	\end{frame}

	\begin{frame}[t]
		
		\begin{itemize}
			
			\item To increase the fidelity of LIMEs, try LIME on discretized input features and on manually constructed interactions.
			
			\item Use cross-validation to construct standard deviations or even confidence intervals for local feature importance values.
			
			\item LIME can fail, particularly in the presence of extreme nonlinearity or high-degree interactions.
			
		\end{itemize}
		
	\end{frame}

	\begin{frame}
	
		\frametitle{Partial Dependence (PD) and Individual Conditional Expectation (ICE)}
		
		\vspace{-10 pt}
		
		\begin{itemize}
			
			\item Following Friedman, Hastie, and Tibshirani (\citeyear{esl}) a single feature $X_j \in \mathbf{X}$ and its complement set $X_{(-j)} \in \mathbf{X}$ (where $X_j \cup X_{(-j)} = \mathbf{X}$) is considered.
			
			\item $\text{PD}(X_j, g)$ for a given feature $X_j$ is estimated as the average output of the learned function $g$ when all the components of $X_j$ are set to a constant $x \in \mathcal{X}$ and $X_{(-j)}$ is left untouched.
			
			\item $\text{ICE}(X_j, \mathbf{x}^{(i)}, g)$ for a given observation $\mathbf{x}^{(i)}$ and feature $X_j$ is estimated as the output of the learned function $g$ when $x^{(i)}_j$ is set to a constant $x \in \mathcal{X}$ and $\mathbf{x}^{(i)} \in X_{(-j)}$ are left untouched.
			
			\item PD and ICE curves are usually plotted over some set of interesting constants $x \in \mathcal{X}$. 
			
		\end{itemize}
		
	\end{frame}
	
	\begin{frame}[t]
		
		\frametitle{Partial Dependence (PD) and Individual Conditional Expectation (ICE)}
		
		\begin{figure}[htb]
			\begin{center}
				\includegraphics[height=100pt]{../img/pdp_ice.png}
				\label{fig:pdp_ice}
				\caption{PD and ICE curves for $ X_j = \text{num}_9$, for known signal generating function $f(\mathbf{X}) = \text{num} _1 * \text{num}_4 + |\text{num}_8| * \text{num}_9^2 + e$, and for machine-learned GBM response function $g$.}
			\end{center}
		\end{figure}
		
		\vspace{-10pt}
		
		Overlaying PD and ICE curves is a succinct method for describing global and local prediction behavior and can be used to detect interactions. (\cite{ice_plots}) 
		
	\end{frame}

	\begin{frame}[t]
		
		\frametitle{Partial Dependence (PD) and Individual Conditional Expectation (ICE)}
		
		\vspace{-20pt}
		
		\begin{figure}[htb]
			\begin{center}
				\label{fig:dt_surrogate_pdp_ice}
				\includegraphics[height=95pt]{../img/dt_surrogate2_pdp_ice2.png}
				\caption{Surrogate DT, PD, and ICE curves for $ X_j = \text{num}_9$, for known signal generating function $f(\mathbf{X}) = \text{num} _1 * \text{num}_4 + |\text{num}_8| * \text{num}_9^2 + e$, and for machine-learned GBM response function $g$.}
			\end{center}
		\end{figure}
		
		Combining Surrogate DT models with PD and ICE curves is a convenient method for detecting, confirming, and understanding important interactions. 
		
	\end{frame}

	%-------------------------------------------------------------------------------
	\section{Why?}
	%-------------------------------------------------------------------------------
	
	\subsection*{} % just for progress indicator

	\begin{frame}
	
		\frametitle{Why Explainable ML?}
		
		Responsible Use of Explainable ML can enable:
		\begin{itemize}
			\item Human learning from machine learning
			\item Human appeal of automated decisions
			\item Regulatory compliance\footnote{\tiny{In the U.S., interpretable models, explanations, and the model documentation they enable may be required under the Civil Rights Acts of 1964 and 1991, the Americans with Disabilities Act, the Genetic Information Nondiscrimination Act, the Health Insurance Portability and Accountability Act, the Equal Credit Opportunity Act, the Fair Credit Reporting Act, the Fair Housing Act, Federal Reserve SR 11-7, and the European Union (EU) General Data Protection Regulation (GDPR) Article 22 \cite{ff_interpretability}.}}
			\item White-hat hacking and security audits of ML models
		\end{itemize}
		\vspace{10pt}
		Even logistic regression is often ``explained'',  or post-processed, for credit scoring, e.g. max. points lost method and adverse action notices.

	\end{frame}

	\begin{frame}[t]
	
		\frametitle{Why Propose Guidelines?}		

		Misuse and Abuse of Explainable ML can enable:
		\begin{itemize}\footnotesize
			\item Model and data stealing (\citet{model_stealing}, \citet{membership_inference}, \citet{shokri2019privacy})
			\item False justification for harmful black-boxes, e.g. ``fairwashing'' (\citet{fair_washing}, \citet{please_stop})
		\end{itemize}
		\vspace{5pt}		
		Explainable ML is already in-use: 
		\begin{itemize}\footnotesize
		\item Numerous open source\footnote{\tiny{\textcolor{magenta}{Please contribute:} \url{https://github.com/jphall663/awesome-machine-learning-interpretability}.}} and commercial packages\footnote{\tiny{For instance Datarobot, H2O Driverless AI, SAS Visual Data Mining and Machine Learning, Zest AutoML.}} are available today.
		\item At least gradient-based feature attribution, partial dependence, and surrogate models are used for model validation in financial services today.\footnote{\tiny{See: \url{https://ww2.amstat.org/meetings/jsm/2019/onlineprogram/AbstractDetails.cfm?abstractid=303053}.}}\textsuperscript{,}\footnote{\tiny{See: Working paper: ``SR 11-7, Validation and Machine Learning Models'', Tony Yang, CFA, CPA, FRM. KPMG USA. \label{fn:yang}}} 
		\end{itemize}
		\vspace{5pt}	
		Regulatory guidance is not agreed upon yet.\footnote{\tiny{See: \url{https://www.americanbanker.com/news/regulators-must-issue-ai-guidance-or-fdic-will-mcwilliams}.}}
		\normalsize	
		
	\end{frame}

	\begin{frame}
	
		\frametitle{Guidelines for Responsible Use of Explainable ML}
		
		\begin{enumerate}\Large
			\item \textbf{Use explainable ML to enhance understanding.}
			\item \textbf{Learn how explainable ML is used for nefarious purposes.}
			\item \textbf{Augment surrogate models with direct explanations.}
			\item \textbf{Use highly transparent mechanisms for high stakes applications (\citet{please_stop}).}
		\end{enumerate}
		
	\end{frame}

%-------------------------------------------------------------------------------
	\section{Understanding and Trust}
%-------------------------------------------------------------------------------

	\subsection*{} % just for progress indicator

	\begin{frame}
	
		\frametitle{1: Use Explainable ML to \textbf{Enhance Understanding}}
		
		\begin{itemize}\Large
			\item{Explanations enhance understanding \textbf{directly}, and increase trust as a \textbf{side-effect}.}
			\vspace{10pt}
			\item{Models can be \textbf{understood and not trusted}, and \textbf{trusted but not understood}.}
			\vspace{10pt}
			\item{Explanations \textbf{alone} are neither necessary nor sufficient for trust.}
			\vspace{10pt}
			\item{Good explanations \textbf{enable human appeal} of model decisions.}
		\end{itemize}
		
	\end{frame}

	\begin{frame}[label={no_trust}]
	
		\frametitle{Understanding Without Trust}
		
					\begin{columns}
				
						\column{0.5\linewidth}
						\centering
						\vspace{7pt}\\
						\includegraphics[height=140pt]{../img/global_shap.png}\\
						\vspace{5pt}
						\tiny{$g_{\text{mono}}$ monotonically-constrained probability of default (PD) classifier trained on the UCI credit card dataset over-emphasizes the most important feature, a customer's most recent repayment status, $\text{PAY\_0}$ \cite{uci}.}

						\vspace{10pt}
						\column{0.6\linewidth}
						\centering
						\includegraphics[height=130pt]{../img/lecture_5.png}\\
						\vspace{5pt}
						\tiny{$g_{\text{mono}}$ also struggles to predict default for favorable statuses, $-2  \leq \texttt{PAY\_0}  < 2$, and often cannot predict on-time payment when recent payments are late, $\text{PAY\_0} \geq 2$}.
				
					\end{columns}
					\normalsize				
		
	\end{frame}
	
	\begin{frame}
	
		\frametitle{Trust Without Understanding}
		\Large
		Years before reliable explanation techniques were widely acknowledged and available, black-box predictive models, such as autoencoder and MLP neural networks, were used for fraud detection in the financial services industry (\citet{gopinathan1998fraud}). When these models performed well, they were trusted.\footnote{\tiny{For example: \url{https://www.sas.com/en_ph/customers/hsbc.html}, \url{https://www.kdnuggets.com/2011/03/sas-patent-fraud-detection.html}}.} However, they were not explainable or well-understood by contemporary standards.  

	\end{frame}


%-------------------------------------------------------------------------------
	\section{The Dark Side}
%-------------------------------------------------------------------------------

	\subsection*{} % just for progress indicator

	\begin{frame}
	
		\frametitle{2: Explainable ML Can be Used for \textbf{Nefarious Purposes}}	
		\Large
		When unintentionally misused, explainable ML can act as a faulty safeguard for potentially harmful black-boxes.\\
		\vspace{10pt}
		When intentionally abused, explainable ML can be used for: 
		\begin{itemize}
			\item Stealing data, models, or other intellectual property.
			\item \textit{Fairwashing}, to mask the sociological biases of a discriminatory black-box.
		\end{itemize}
	
	\end{frame}

	\begin{frame}
	
		\frametitle{\textit{\textbf{AI Incident}}: ML Hacking}
		
				\footnotesize{Many ML hacks use, or are exacerbated by, explainable ML techniques.}\footnote{\tiny{Source: O'Reilly Media, \href{http://info.h2o.ai/rs/644-PKX-778/images/OReilly_Responsible_ML_eBook.pdf}{\textit{Responsible Machine Learning}}, Patrick Hall, Navdeep Gill, and Benjamin Cox.}
}				\begin{figure}
					\begin{center}
						\includegraphics[height=160pt]{../img/Attack_Cheat_Sheet.png}
					\end{center}
				\end{figure}	
				\normalsize
	
	\end{frame}
	
	\begin{frame}
	
		\frametitle{\large{\textbf{Case 2.1}: White-hat Attacks Can Crack Potentially Harmful Black-boxes}}
		\large
		The flip-side of the dark side is community oversight of black-boxes.\\
		\vspace{10pt}
		Recent high profile analyses of commercial black-boxes, e.g. ... 
		\begin{itemize}		
			\item \href{https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing}{Propublica and COMPAS} (\citet{angwin16})\footnote{\tiny{This presentation makes no claim on the quality of the analysis in Angwin et al. (2016), which has been criticized, but is simply stating that such cracking is possible \cite{angwin16,}, \cite{flores2016false}}.}
			\item \href{https://medium.com/@Joy.Buolamwini/response-racial-and-gender-bias-in-amazon-rekognition-commercial-ai-system-for-analyzing-faces-a289222eeced}{Gendershades and Rekognition} (\citet{gender_shades}, \citet{raji2019actionable})
		\end{itemize}
		... \textbf{could} be characterized as white-hat attacks on proprietary black-boxes (respectively, model stealing and adversarial examples).

	\end{frame}
	
	\begin{frame}[label={not_frontline}]
	
		\frametitle{\textbf{Case 2.2}: Explanation \textbf{\textit{is Not}} a Front Line Fairness Tool}
		\large
		Use fairness tools, e.g. ...
		\vspace{5pt}
		\begin{itemize}\normalsize
			\item Disparate impact testing (\citet{feldman2015certifying})
			\item Reweighing (\citet{kamiran2012data})
			\item Reject option based classification (\citet{kamiran2012decision})
			\item Adversarial de-biasing (\citet{zhang2018mitigating})
			\item \href{https://github.com/dssg/aequitas}{aequitas}, \href{https://github.com/IBM/AIF360}{AIF360}, \href{https://github.com/LASER-UMASS/Themis}{Themis}, \href{https://github.com/cosmicBboy/themis-ml}{themis-ml}
		\end{itemize}
		\vspace{5pt}
		... for fairness tasks: bias testing, bias remediation, and to establish trust.\\
		\vspace{10pt}
		Explanations can be used to understand and augment such results.
		
	\end{frame}
	
%-------------------------------------------------------------------------------
	\section{Surrogates}
%-------------------------------------------------------------------------------

	\subsection*{} % just for progress indicator
	
	\begin{frame}[t]
	
		\frametitle{3: Augment \textbf{Surrogate Models} with \textbf{Direct Explanations}}	
	
		\begin{columns}
				
			\column{0.55\linewidth}
			\centering			
			\includegraphics[height=0.6\linewidth, width=.95\linewidth]{../img/figure_3-eps-converted-to.png}\\
  			\tiny{Na\"ive $h_{\text{tree}}$, \textit{a surrogate model}, forms an approximate overall flowchart for the explained model, $g_{\text{GBM}}$.}

			\hspace{5pt}
			\column{.45\textwidth}
  			\includegraphics[height=.52\linewidth, width=1.02\linewidth]{../img/figure_4.png}\\
  			\tiny{Partial dependence and ICE curves generated \textit{directly from the explained model}, $g_{\text{GBM}}$.}
  			
  		\end{columns}

	\scriptsize{$h_{\text{tree}}$ displays known interactions in $f = X_{\text{num}1} * X_{\text{num}4} + |X_{\text{num}8}| * X_{\text{num}9}^2$ for $\sim -0.923 < X_{\text{num9}} <  \sim 1.04$. Modeling of the known interaction between $X_{\text{num9}}$ and $X_{\text{num8}}$ in $f$ by $g_{\text{GBM}}$ is also highlighted by the divergence of partial dependence and ICE curves for $\sim -1 < X_{\text{num9}} <  \sim 1$.}

	\end{frame}

	\begin{frame}[t, label={lime}]
	
		\frametitle{\textbf{Example 3.1}: Augment LIME with Direct Explanations}
	
		\begin{columns}
				
			\column{0.5\linewidth}
			\centering			
			\includegraphics[height=0.6\linewidth, width=.95\linewidth]{../img/shap_blue.png}\\
  			\tiny{Locally accurate Shapley contributions for a high risk individual's probability of default as predicted by a simple decision tree model, $g_{\text{tree}}$. See slide~\ref{dt} for a directed graph representation of $g_{\text{tree}}$}.

			\hspace{5pt}
			\column{0.5\linewidth}
			\begin{table}
				\centering
				\tiny
				\begin{tabular}{ | p{2cm} | p{1.7cm} | }
					\hline
					$h_{\text{GLM}}$\newline Feature & $h_{\text{GLM}}$\newline Coefficient \\ 
					\hline
					\texttt{PAY\_0 == 4} & $0.0009$ \\
					\hline
					\texttt{PAY\_2 == 3} & $0.0065$ \\
					\hline
					\texttt{PAY\_6 == 2} & $0.0036$ \\
					\hline
					\texttt{PAY\_5 == 2} & $-0.0006$ \\
					\hline					
					\texttt{PAY\_AMT1} & $4.8062\mathrm{e}{-07}$ \\
					\hline										
					\texttt{BILL\_AMT1} & $3.4339\mathrm{e}{-08}$ \\
					\hline
					\texttt{PAY\_AMT3} & $-5.867\mathrm{e}{-07}$ \\	
					\hline	
				\end{tabular}	
			\end{table}	
  			\tiny{Coefficients for a local linear interpretable model, $h_{\text{GLM}}$, with an intercept of 0.77 and an $R^2$ of 0.73, trained between the original inputs and predictions of $g_{\text{tree}}$ for a segment of the UCI credit card dataset with late most recent repayment statuses, $\mathbf{X}_{PAY \_ 0 > 1}$}.
  		\end{columns}

	\scriptsize{Because $h_{GLM}$ is relatively well-fit and has a logical intercept, it can be used along with Shapley values to reason about the modeled average behavior for risky customers and to differentiate the behavior of any one specific risky customer from their peers under the model.}

	\end{frame}


%-------------------------------------------------------------------------------
	\section{High Stakes Applications}
%-------------------------------------------------------------------------------

	\subsection*{} % just for progress indicator

	\begin{frame}
	
		\frametitle{4: Use Highly Transparent Mechanisms for \textbf{High Stakes Applications}}
		\vspace{2pt}
		\centering{\includegraphics[scale=0.09]{../img/rml_diagram_lec2_hilite.png}}\\
		\vspace{5pt}
		\scriptsize{A diagram of a proposed workflow in which \colorbox{pink}{explanations} are used along with interpretable models, disparate impact analysis and remediation techniques, and other review and appeal mechanisms to create a fair, accountable, and transparent ML system.}
	
	\end{frame}

	\begin{frame}

% XNN

		\frametitle{\normalsize{\textbf{Case 4.1}: Use \textbf{Interpretable Models} for High Stakes Applications (\citet{please_stop})}}
		
		In addition to penalized GLM, decision trees, and conventional rule-based models, many other types of accurate and interpretable models are available today, e.g. ...
		
		\begin{itemize}
			\item \href{https://github.com/microsoft/interpret}{Explainable boosting machine (EBM)}
			\item Monotonic GBM in \href{https://github.com/h2oai/h2o-3}{h2o} or \href{https://github.com/dmlc/xgboost}{XGBoost}
			\item \href{https://cran.r-project.org/web/packages/pre/index.html}{RuleFit} (\citet{rulefit})
			\item \href{https://github.com/ustunb/slim-python}{Super-sparse linear integer model} (SLIM)	(\citet{slim})
			\item Explainable neural network (XNN) (\citet{wf_xnn})
			\item \href{https://cran.r-project.org/web/packages/sbrl/index.html}{Scalable Bayesian rule list} (\citet{sbrl})
		\end{itemize}	

		... use them for human-centered or other high stakes ML applications.\footnote{\tiny{There are shades of interpretability in models. Interpretability is probably not a binary, on-off quality. For instance see Figure 3: \url{https://arxiv.org/pdf/1904.03867.pdf} \cite{molnar2019quantifying}.}}
	
	\end{frame}	

	\begin{frame}[label={dt}]
	
		\frametitle{\normalsize\textbf{Case 4.2}: Explanations and Interpretable Models are \textbf{Not Mutually Exclusive}}
		
		\begin{columns}
				
			\column{0.5\linewidth}
			\centering		
			\includegraphics[height=.45\linewidth, width=1.15\linewidth]{../img/dt.png}\\
			\vspace{5pt}
  			\tiny{Simple decision tree, $g_{\text{tree}}$, trained on the UCI credit card data to predict default with validation AUC of 0.74. The decision policy for high risk individuals is highlighted in \textcolor{fuschia}{fuschia}.}

			\hspace{50pt}
			\column{.4\textwidth}
			\centering
			\vspace{2pt}\\
  			\includegraphics[height=.5\linewidth, width=.8\linewidth]{../img/shap_blue.png}\\
  			\vspace{5pt}
  			\tiny{Locally accurate Shapley contributions for the highlighted individual's probability of default. See slide \ref{lime} for LIMEs for the high risk customers in $g_{\text{tree}}$.}

		\end{columns}
		\vspace{10pt}

	\scriptsize{The Shapley values are helpful because they highlight the local importance of features not on the decision path, which could be underestimated by examining the decision policy alone.}
	
	\end{frame}
	
	\begin{frame}
		
		\frametitle{\textbf{Interlude}: An Ode to the Shapley Value}		
		
		\begin{enumerate}\footnotesize
				
			\item \textbf{In the beginning}: \citefield{shapley1953value}{title}, \citefield{shapley1953value}{year} \cite{shapley1953value}
			\item \textbf{Nobel-worthy contributions}: \citefield{shapley1988shapley}{title}, \citefield{shapley1988shapley}{year} \cite{shapley1988shapley}
			\item \textbf{Shapley regression}: \citefield{lipovetsky2001analysis}{title}, \citefield{lipovetsky2001analysis}{year} \cite{lipovetsky2001analysis}
			\item \textbf{First reference in ML?} \citefield{keinan2004fair}{title}, \citefield{keinan2004fair}{year} \cite{keinan2004fair}
			\item \textbf{Into the ML research mainstream, i.e. JMLR}: \citefield{kononenko2010efficient}{title}, \citefield{kononenko2010efficient}{year} \cite{kononenko2010efficient}
			\item \textbf{Into the real-world data mining workflow ... \textit{finally}}: \citefield{tree_shap}{title}, \citefield{tree_shap}{year}\footnote{\tiny{See \href{https://github.com/h2oai/h2o-3}{h2o}, \href{https://github.com/microsoft/LightGBM}{LightGBM}, or \href{https://github.com/dmlc/xgboost}{XGBoost} for implementation.}} \cite{tree_shap} 
			\item \textbf{Unification}: \citefield{shapley}{title}, \citefield{shapley}{year}\footnote{\tiny{See \href{https://github.com/slundberg/shap}{shap} for implementation.}} \cite{shapley}
				
		\end{enumerate}
			
	\end{frame}
	
	\begin{frame}
	
		\frametitle{\large{\textbf{Case 4.3}: Explanation and Fairness Techniques are \textbf{Not Mutually Exclusive}}}
		
		
				\begin{columns}
			
					\column{0.5\linewidth}
					\includegraphics[height=.8\linewidth]{../img/fi_shortfall.png}\\
					\vspace{5pt}
					\scriptsize{ML model input features plotted by Shapley importance and shortfall to equality -- a basic fairness metric.}
		
		%\begin{table}
		%	\centering
		%	\scriptsize
		%	\begin{tabular}{ | p{1.2cm} | p{1.1cm} | p{1.3cm} | p{1.2cm}| p{1.2cm} | p{1.2cm} | p{1.2cm} | p{1.2cm} | }
		%		\hline
		%		& Adverse\newline Impact\newline Disparity & Accuracy Disparity & TPR\newline Disparity & TNR\newline Disparity & FPR\newline Disparity & FNR\newline Disparity \\ 
		%		\hline	
		%		\texttt{single} & 0.89 & 1.03 & 0.99 & 1.03 & 0.85 & 1.01 \\
		%		\hline	
		%		\texttt{divorced} & 1.01 & 0.93 & 0.81 & 0.96 & \textcolor{fuschia}{1.25} & 1.22 \\
		%		\hline
		%		\texttt{other} & \textcolor{fuschia}{0.26} & 1.12 & \textcolor{fuschia}{0.62} & 1.17 & \textcolor{fuschia}{0} & \textcolor{fuschia}{1.44} \\
		%		\hline	
		%	\end{tabular}
		%\end{table}
		%\tiny{Basic group disparity metrics across different marital statuses for monotonically constrained GBM model, $g_{\text{mono}}$, trained on the UCI credit card dataset.	 See slide \ref{no_trust} for global Shapley feature importance for $g_{\text{mono}}$ and slide \ref{not_frontline} for an important note about explanation and fairness techniques.}\\
		%\vspace{5pt}\footnotesize

		
				\column{0.5\linewidth}
				Many fairness toolkits are available today: \href{https://github.com/dssg/aequitas}{aequitas}, \href{https://github.com/IBM/AIF360}{AIF360}, \href{https://github.com/LASER-UMASS/Themis}{Themis}, \href{https://github.com/cosmicBboy/themis-ml}{themis-ml}.\\
				\vspace{5pt}
				These can be mixed with explanation techniques, such as Shapley values, to great benefit. See  \href{https://bit.ly/2R3dXlu}{
					Explaining quantitative measures of fairness} for an example. 
				\\
				\vspace{5pt}
				Traditional disparate impact testing tools are best-suited for constrained models because average group metrics cannot reliably identify local instances of discrimination that can occur when using complex, unconstrained models.   
				
			\end{columns}
	
	\end{frame}

%-------------------------------------------------------------------------------
		
\section{Acknowledgements} 

	\begin{frame}
	
		\frametitle{Acknowledgments}
		
		Some of the best engineers, researchers, and business leaders in the world!\\
		\vspace{10pt}
		Christoph Molnar, Doug Deloy, Josephine Wang, Kerry O'Shea, Ladislav Ligart, Leland Wilkinson, Mark Chan, Martin Dvorak, Mateusz Dymczyk, Megan and Michal Kurka, Mike Williams, Navdeep Gill, Pramit Choudhary, Przemyslaw Biecek, Sameer Singh, Sri Ambati, Wen Phan, Zac Taschdjian, and Lisa Song.\footnote{\tiny{My world anyway ... and in alphabetical order by first name.}}\\
		\vspace{10pt}
		Some materials \copyright\hspace{1pt}Patrick Hall and the H2O.ai team 2017-2020. 

	\end{frame}	
			
%-------------------------------------------------------------------------------
% References
%-------------------------------------------------------------------------------


	\begin{frame}[t, allowframebreaks]
	
		\frametitle{References}	
		
			\normalsize Code examples for this presentation:\\
			\scriptsize{\url{https://www.github.com/jphall663/interpretable_machine_learning_with_python}}\\
			\noindent\scriptsize{\url{https://www.github.com/jphall663/responsible_xai}}\\
			\vspace{10pt}
			\normalsize Associated texts:\\
			\scriptsize{\url{https://arxiv.org/pdf/1810.02909.pdf}}\\
			\noindent\scriptsize{\url{https://arxiv.org/pdf/1906.03533.pdf}}
								
		\framebreak		
		
		\tiny
		\printbibliography
		
	\end{frame}

\end{document}