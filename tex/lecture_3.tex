\documentclass[11pt,aspectratio=169,hyperref={colorlinks}]{beamer}
\usetheme{Singapore}
\usecolortheme[snowy, cautious]{owl}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[american]{babel}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    urlcolor=[rgb]{0,0,0.61},
    linkcolor=[rgb]{0,0,0.61}}
\usepackage[natbib=true,style=numeric,backend=bibtex,useprefix=true]{biblatex}

%----------------------------------------------------------------------------------
\definecolor{OwlGreen}{RGB}{51,0,102} 
%----------------------------------------------------------------------------------

\setbeamertemplate{bibliography item}{}
\setbeamerfont{caption}{size=\footnotesize}
\setbeamertemplate{frametitle continuation}{}
\setcounter{tocdepth}{1}
\renewcommand*{\bibfont}{\scriptsize}
\addbibresource{lecture_3.bib}

%------------------------------------------------------------------------------------------

\usenavigationsymbolstemplate{}
\setbeamertemplate{footline}{%
    \raisebox{5pt}{\makebox{\hfill\makebox[20pt]{\color{gray}
          \scriptsize\insertframenumber}}}\hspace*{5pt}}

%------------------------------------------------------------------------------------------
\renewcommand*{\thefootnote}{\fnsymbol{footnote}}
%------------------------------------------------------------------------------------------

\author{Patrick Hall}
\title{Responsible Machine Learning\footnote{\tiny{This material is shared under a \href{https://creativecommons.org/licenses/by/4.0/deed.ast}{CC By 4.0 license} which allows for editing and redistribution, even for commercial purposes. However, any derivative work should attribute the author.}}}
\subtitle{Lecture 3: Discrimination Testing and Remediation}
\institute{The George Washington University}
\date{\today}


\begin{document}
	
	\maketitle
	
	\begin{frame}
	
		\frametitle{Contents}
		
		\tableofcontents{}
		
	\end{frame}


%-------------------------------------------------------------------------------
	\section{Introduction}
%-------------------------------------------------------------------------------

		\subsection*{}

		\begin{frame}
		
			\frametitle{A Responsible Machine Learning Workflow\footnote{\href{https://www.mdpi.com/2078-2489/11/3/137/htm}{\textit{A Responsible Machine Learning Workflow}}}}
			
			\begin{figure}[htb]
				\begin{center}
					\includegraphics[height=150pt]{../img/rml_diagram_lec3_hilite.png}
					\label{fig:blueprint}
				\end{center}
			\end{figure}		
					
		\end{frame}	

		\begin{frame}				
		
			\frametitle{Why Care About Discrimination in Machine Learning?}
			
			\begin{itemize}
				\Large
				\item \textbf{Responsible practice of machine learning (ML)}: ML can affect millions of people! \cite{obermeyer2019dissecting}
				\item \textbf{Discrimination is often illegal (in the U.S.)}: Non-compliance fines and litigation costs.
				\item \textbf{Reputational risk}: Upon encountering a perceived unethical ML system, 34\% of consumers are likely to, ``stop interacting with the company.''\footnote{\scriptsize{See: \href{https://www.capgemini.com/research/why-addressing-ethical-questions-in-ai-will-benefit-organizations/}{Why addressing ethical questions in AI will benefit organizations}.}}
			\end{itemize}
			
		\end{frame}

%-------------------------------------------------------------------------------
	\section{Bias and Discrimination}
%-------------------------------------------------------------------------------
	
		\subsection*{}
	
		\begin{frame}
		
			\frametitle{What Is Bias?}			
					
			\begin{itemize}
			\item Almost \textit{all} data, statistical models, and ML models encode different types of \textit{bias}, i.e., \textbf{systematic misrepresentations of reality}.\\
			\item Sometimes, bias is \textit{helpful}.
				\begin{itemize}
					\item{Shrunken and robust $\beta_j$ coefficients in penalized linear models.}	\end{itemize}
			\item Other types of bias can be unwanted, unhelpful, discriminatory, or illegal. 
			\item Many instances of discrimination in ML arise from sociologically biased experimental design, data collection, labeling, or storage processes.
			\end{itemize}
					
		\end{frame}					
		
		\begin{frame}				
			
			\frametitle{What is Discrimination in ML?}
			
			\noindent In many applications\footnote{\small{e.g., Under the Equal Credit Opportunity Act (ECOA), as implemented by Regulation B, and the Fair Credit Reporting Act (FCRA})}, model predictions should \textbf{\textit{ideally}} be independent of demographic group membership.\\
			\vspace{5pt}
			\noindent In these applications, a model exhibits discrimination if:
			\begin{enumerate}
				\item Demographic group membership is not independent of the likelihood of receiving a favorable or accurate model prediction.
				\item Membership in a \textit{subset} of a demographic group is not independent of the likelihood of receiving a favorable or accurate model prediction (i.e., \textit{local or individual discrimination}).\cite{hall2019guidelines}
			\end{enumerate}
		
		\end{frame}
		
		\begin{frame}
		
			\frametitle{What Kinds of Discrimination Occur in ML?}	
			
			\noindent \Large Several forms of discrimination may manifest in ML, including:
			\begin{itemize}
				
				\item Group disparities: 
				
				\begin{itemize}
				
					\item Overt discrimination against groups, i.e., \textit{disparate treatment} (DT).
					
					% More careful definition: Disparate treatment occurs when a lender treats a (potential) customer differently based on a prohibited basis, such as the person’s age, gender, or race.
					
					\item Unintentional discrimination against groups, i.e., \textit{disparate impact} (DI).
					
					% More careful definition: Disparate impact occurs when a protected class experiences a larger share of less favorable outcomes as a result of an otherwise non-discriminatory and legitimate decision-making process. Disparate impact is not necessarily a violation of law and may be justified by a “business necessity,” such as cost or profitability. However, there may still be a violation if the lenders could have used an alternative policy or practice that had a less discriminatory effect.
					
					% All DI is discrimination, I'm not sure that all discrimination is DI.
					
					\item Differing quality across demographic groups, i.e., \textit{differential validity}.
				
				\end{itemize}
				
				\item Local or individual discrimination.
				
			\end{itemize}
		
		\end{frame}			
		
		\begin{frame}				
			
			\frametitle{How Does Discrimination Arise in ML?}
			
			\noindent Discrimination originates from poor experimental design:\\
			\begin{itemize}
				\item Asking biased questions, e.g., “can a face predict trustworthiness?”, “can demographics predict creditworthiness?”
				\item Modeling biased phenomenon, e.g., healthcare spending vs. healthcare need. 
			\end{itemize}
			
			\noindent Discrimination originates from training data:\\
			\begin{itemize}
				\item Incomplete or inaccurate data, e.g., under-representation of minorities. See \href{http://gendershades.org/}{Gender Shades} \cite{gender_shades}.
				\item Accurate but differing patterns of causation, correlation, or dependency between demographic groups and past outcomes, e.g., traditional FICO credit scores.\footnote{\scriptsize{See: \url{https://shiftprocessing.com/credit-score/\#race}}}
			\end{itemize}

		\end{frame}

		\begin{frame}				
		
			\frametitle{How Does Discrimination Arise in ML?}
			
			ML models can perpetuate or exacerbate discrimination.\\
			\vspace{10pt}
			\noindent \textbf{Group disparities}, i.e., different or inaccurate treatment of entire demographic groups:\\
			\begin{itemize}
				\item Including direct or proxy identifiers for demographic group membership, i.e., \textit{DT}.
				\item Learning different correlations between demographic groups and favorable model outcomes, i.e., \textit{DI}.
				\item Exhibiting different accuracies across demographic groups, i.e., \textit{differential validity}.\textsuperscript{$\mathparagraph$}
			\end{itemize}
			\vspace{5pt}
			\noindent \textbf{Locally}, i.e., different or inaccurate treatment of similar individuals:\\
			\begin{itemize}
				\item Local response function or decision boundary form. 
				\item Capacity to form local complex demographic proxies on a row-by-row basis.
			\end{itemize}
							
		\end{frame}

%-------------------------------------------------------------------------------
	\section{Testing for Discrimination in ML}
%-------------------------------------------------------------------------------

		\begin{frame}				
		
			\frametitle{Common Metrics of Discrimination in ML}
			
			Common metrics for DI and \textbf{\textit{group}} disparities:\\
			\begin{itemize}
				\item Differential validity: $\frac{\text{quality}_p}{\text{quality}_r}$
				\item Adverse impact ratio: $\frac{\text{\% accepted}_p }{ \text{\% accepted}_r}$ 
				\item Marginal effect: $\text{\% accepted}_p - \text{\% accepted}_r$
				\item Standardized mean difference: $\frac{\bar{\hat{y}}_p - \bar{\hat{y}}_r}{\sigma_{\hat{y}}}$
			\end{itemize}
			\noindent 
			\scriptsize{where, $p \equiv \text{protected group}$ and $r \equiv \text{reference group}$ (often white males).}\\
			\vspace{5pt}
			\normalsize{There are many other, sometimes conflicting, mathematical definitions of discrimination. 
				See \href{https://www.youtube.com/watch?v=wqamrPkF5kk}{21 Definitions of Fairness and Their Politics}.}
			
		\end{frame}
	
		\begin{frame}				
	
			\frametitle{Additional Considerations for Discrimination Testing}
	
			\begin{itemize}
				
				\item Local discrimination, i.e., the model treats a small number of similar people differently. 
				
				\begin{itemize}
					\item Constrain problematic interactions.
					\item Search around probability thresholds.
					\item Adversarial models.
				\end{itemize}
			
				\item Post-hoc explanation to understand drivers of discrimination:
				\begin{itemize}
					
					\item To be conducted after discrimination is confirmed by standard tests.
					
					\item Be aware of: 
					\begin{itemize}
						\item No demographic features in model.
						\item Fairwashing \cite{fair_washing} and scaffolding \cite{scaffolding}. 
					\end{itemize}
				
				\end{itemize}
	
			\end{itemize}
	
		\end{frame}
	

%-------------------------------------------------------------------------------
	\section{Remediation}
%-------------------------------------------------------------------------------
	
		\subsection*{}

		\begin{frame}			
		
			\frametitle{How to Fix Discrimination in ML?}
			\Large \noindent 
			\textbf{Fix organizational processes}: Lecture 6\\
			\noindent 
			\textbf{Fix the data}:
			\begin{itemize}
				\item Collect demographically representative training data.
				\item Label and annotate data carefully.
				\item Select features judiciously.
				\item Sample and reweigh training data to minimize discrimination.\cite{kamiran2012data}
			\end{itemize}
			
		\end{frame}	
		
			
		\begin{frame}			
		
			\frametitle{How to Fix Discrimination in ML?}			
			
			\noindent 
			\textbf{Fix the model}:
			\begin{itemize}
				\item Consider fairness metrics when selecting hyperparameters and cutoff thresholds.
				\item Train fair models directly:
				\begin{itemize}
					\item Learning fair representations (LFR) and adversarial de-biasing.\cite{zemel2013learning}, \cite{zhang2018mitigating}
					\item Use dual objective functions that consider both accuracy and fairness metrics.
				\end{itemize}
				\item Edit model mechanisms to ensure less biased predictions, e.g., with \href{https://github.com/interpretml/interpret}{GA2M/EBM} models.
			\end{itemize}		
			\noindent \textbf{Fix the predictions}: 
			\begin{itemize}
				\item Balance model predictions, e.g., reject-option classification.\cite{kamiran2012decision}	
				\item Correct or override predictions with model assertions or appeal mechansims.\cite{hall2019guidelines}, \cite{kangdebugging}
			\end{itemize}
			
		\end{frame}
	
		\begin{frame}
	
			\frametitle{How to Fix Discrimination in ML?}		
			\centering
			Consider discrimination measures during model selection.\\
			\vspace{10pt}
			{\includegraphics[scale=0.09]{../img/lecture_3.png}}
			
		\end{frame}	
		
		\begin{frame}
		
			\frametitle{How to Fix Discrimination in ML?}		
			\centering
			As part of a responsible ML workflow.\\
			\vspace{10pt}
			{\includegraphics[scale=0.08]{../img/rml_diagram_no_hilite.png}}
			
		\end{frame}		
		
%-------------------------------------------------------------------------------
	\section{Acknowledgements}
%-------------------------------------------------------------------------------

	\subsection*{}
	
	\begin{frame}
	
		\frametitle{Acknowledgements}
		
		This presentation borrows heavily from the expertise of Nicholas Schmidt and Bryce Stephens of \href{https://www.bldsllc.com/}{BLDS, LLC}, a leading fair lending compliance firm.\\
		\vspace{10pt}		
		Thanks to Lisa Song for her continued assistance in developing these course materials.\\
		\vspace{10pt}
		Some materials \copyright\hspace{1pt}Patrick Hall and the H2O.ai team 2017-2020.  

	\end{frame}	
	
%-------------------------------------------------------------------------------
%	References
%-------------------------------------------------------------------------------

	\begin{frame}[t, allowframebreaks]
	
		\frametitle{References}
		\printbibliography
		
	\end{frame}

\end{document}