\documentclass[11pt,aspectratio=169,hyperref={colorlinks}]{beamer}
\usetheme{Singapore}
\usecolortheme[snowy, cautious]{owl}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[american]{babel}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    urlcolor=[rgb]{0,0,0.61},
    linkcolor=[rgb]{0,0,0.61}}
\usepackage[natbib=true,style=numeric,backend=bibtex,useprefix=true]{biblatex}

%----------------------------------------------------------------------------------
\definecolor{OwlGreen}{RGB}{51,0,102} 
%----------------------------------------------------------------------------------

\setbeamertemplate{bibliography item}{}
\setbeamerfont{caption}{size=\footnotesize}
\setbeamertemplate{frametitle continuation}{}
\setcounter{tocdepth}{1}
\renewcommand*{\bibfont}{\scriptsize}
\addbibresource{lecture_3.bib}

%------------------------------------------------------------------------------------------

\usenavigationsymbolstemplate{}
\setbeamertemplate{footline}{%
    \raisebox{5pt}{\makebox{\hfill\makebox[20pt]{\color{gray}
          \scriptsize\insertframenumber}}}\hspace*{5pt}}

%------------------------------------------------------------------------------------------
\renewcommand*{\thefootnote}{\fnsymbol{footnote}}
%------------------------------------------------------------------------------------------

\author{Patrick Hall}
\title{Responsible Machine Learning\footnote{\tiny{This material is shared under a \href{https://creativecommons.org/licenses/by/4.0/deed.ast}{CC By 4.0 license} which allows for editing and redistribution, even for commercial purposes. However, any derivative work should attribute the author.}}}
\subtitle{Lecture 3: Discrimination Testing and Remediation}
\institute{The George Washington University}
\date{\today}


\begin{document}
	
	\maketitle
	
	\begin{frame}
	
		\frametitle{Contents}
		
		\tableofcontents{}
		
	\end{frame}


%-------------------------------------------------------------------------------
	\section{Introduction}
%-------------------------------------------------------------------------------

		\subsection*{}

		\begin{frame}
		
			\frametitle{A Responsible Machine Learning Workflow\footnote{\href{https://www.mdpi.com/2078-2489/11/3/137/htm}{\textit{A Responsible Machine Learning Workflow}}}}
			
			\begin{figure}[htb]
				\begin{center}
					\includegraphics[height=150pt]{../img/rml_diagram_lec3_hilite.png}
					\label{fig:blueprint}
				\end{center}
			\end{figure}		
					
		\end{frame}	

		\begin{frame}				
		
			\frametitle{Why Care About Discrimination in Machine Learning?}
			
			\begin{itemize}
				\Large
				\item \textbf{Responsible practice of machine learning (ML)}: ML can affect millions of people! \cite{obermeyer2019dissecting}
				\item \textbf{Discrimination is often illegal (in the U.S.)}: Non-compliance fines and litigation costs.
				\item \textbf{Reputational risk}: Upon encountering a perceived unethical ML system, 34\% of consumers are likely to, ``stop interacting with the company.''\footnote{\scriptsize{See: \href{https://www.capgemini.com/research/why-addressing-ethical-questions-in-ai-will-benefit-organizations/}{Why addressing ethical questions in AI will benefit organizations}.}}
			\end{itemize}
			
		\end{frame}

%-------------------------------------------------------------------------------
	\section{Bias and Discrimination}
%-------------------------------------------------------------------------------
	
		\subsection*{}
	
		\begin{frame}
		
			\frametitle{What Is Bias?}			
					
			\begin{itemize}
			\item Almost \textit{all} data, statistical models, and ML models encode different types of \textit{bias}, i.e., \textbf{systematic misrepresentations of reality}.\\
			\item Sometimes, bias is \textit{helpful}.
				\begin{itemize}
					\item{Shrunken and robust $\beta_j$ coefficients in penalized linear models.}	\end{itemize}
			\item Other types of bias can be unwanted, unhelpful, discriminatory, or illegal. 
			\item Many instances of discrimination in ML arise from sociologically biased data collection, labeling, or storage processes.
			\end{itemize}
					
		\end{frame}					
		
		\begin{frame}				
			
			\frametitle{What is Discrimination in ML?}
			
			\noindent In many applications\footnote{\small{e.g., Under the Equal Credit Opportunity Act (ECOA), as implemented by Regulation B, and the Fair Credit Reporting Act (FCRA})}, model predictions should \textbf{\textit{ideally}} be independent of demographic group membership.\\
			\vspace{5pt}
			\noindent In these applications, a model exhibits discrimination if:
			\begin{enumerate}
				\item Demographic group membership is not independent of the likelihood of receiving a favorable or accurate model prediction.
				\item Membership in a \textit{subset} of a demographic group is not independent of the likelihood of receiving a favorable or accurate model prediction (i.e., \textit{local or individual discrimination}).\cite{hall2019guidelines}
			\end{enumerate}
		
		\end{frame}
		
		\begin{frame}
		
			\frametitle{What Kinds of Discrimination Occur in ML?}	
			
			\noindent \Large Several forms of discrimination may manifest in ML, including:
			\begin{itemize}
				
				\item Group disparities: 
				
				\begin{itemize}
				
					\item Overt discrimination against groups, i.e., \textit{disparate treatment}.
					
					% More careful definition: Disparate treatment occurs when a lender treats a (potential) customer differently based on a prohibited basis, such as the person’s age, gender, or race.
					
					\item Unintentional discrimination against groups, i.e., \textit{disparate impact} (DI).
					
					% More careful definition: Disparate impact occurs when a protected class experiences a larger share of less favorable outcomes as a result of an otherwise non-discriminatory and legitimate decision-making process. Disparate impact is not necessarily a violation of law and may be justified by a “business necessity,” such as cost or profitability. However, there may still be a violation if the lenders could have used an alternative policy or practice that had a less discriminatory effect.
					
					% All DI is discrimination, I'm not sure that all discrimination is DI.
				
				\end{itemize}
				
				\item Local or individual discrimination.
				
			\end{itemize}
		
		\end{frame}			
		
		\begin{frame}				
		
			\frametitle{How Does Discrimination Arise in ML?}
			
			\noindent Discrimination originates from training data:\\
			\begin{itemize}
				\item Incomplete or inaccurate data, e.g., under-representation of minorities. See \href{http://gendershades.org/}{Gender Shades} \cite{gender_shades}.
				\item Accurate but differing patterns of causation, correlation, or dependency between demographic groups and past outcomes, e.g., traditional FICO credit scores.\footnote{\scriptsize{See: \href{https://www.youtube.com/watch?v=rToFuhI6Nlw}{Responsible Data Science: Identifying and Fixing Biased AI}.}}
				\item Explicit encoding of historical social biases into training data, e.g., criminal records.\textsuperscript{$\mathparagraph$}
			\end{itemize}

		\end{frame}

		\begin{frame}				
		
			\frametitle{How Does Discrimination Arise in ML?}
			
			ML models can perpetuate or exacerbate discrimination.\\
			\vspace{10pt}
			\noindent \textbf{Group disparities}, i.e., different or inaccurate treatment of entire demographic groups:\\
			\begin{itemize}
				\item Learning different correlations between demographic groups and favorable model outcomes, i.e., \textit{DI}.
				\item Exhibiting different accuracies across demographic groups, i.e., \textit{differential validity}.\textsuperscript{$\mathparagraph$}
			\end{itemize}
			\vspace{5pt}
			\noindent \textbf{Locally}, i.e., different or inaccurate treatment of similar individuals:\\
			\begin{itemize}
				\item Local response function or decision boundary form. 
				\item Capacity to form local complex demographic proxies on a row-by-row basis.
			\end{itemize}
							
		\end{frame}

%-------------------------------------------------------------------------------
	\section{Testing for Discrimination in ML}
%-------------------------------------------------------------------------------

		\begin{frame}				
		
			\frametitle{Common Metrics of Discrimination in ML}
			
			Common metrics for DI and \textbf{\textit{group}} disparities:\\
			\begin{itemize}
				\item Accuracy disparity: $\frac{\text{accuracy}_p}{\text{accuracy}_r}$
				\item Adverse impact ratio: $\frac{\text{\% accepted}_p }{ \text{\% accepted}_r}$ 
				\item Marginal effect: $\text{\% accepted}_p - \text{\% accepted}_r$
				\item Standardized mean difference: $\frac{\bar{\hat{y}}_p - \bar{\hat{y}}_r}{\sigma_{\hat{y}}}$
			\end{itemize}
			\noindent 
			\scriptsize{where, $p \equiv \text{protected group}$ and $r \equiv \text{reference group}$ (often white males).}\\
			\vspace{5pt}
			\normalsize{There are many other, sometimes conflicting, mathematical definitions of discrimination. 
				See \href{https://www.youtube.com/watch?v=wqamrPkF5kk}{21 Definitions of Fairness and Their Politics}.}
			
		\end{frame}
	
		\begin{frame}				
	
			\frametitle{Additional Considerations for Discrimination Testing}
	
			\begin{itemize}
				
				\item Local discrimination, i.e., the model treats a small number of similar people differently. 
				
				\begin{itemize}
					\item Search around probability thresholds.
					\item Adversarial models.
				\end{itemize}
			
				\item Post-hoc explanation to understand drivers of discrimination:
				\begin{itemize}
					
					\item To be conducted after discrimination is confirmed by standard tests.
					
					\item Be aware of: 
					\begin{itemize}
						\item No demographic features in model.
						\item Fairwashing \cite{fair_washing} and scaffolding \cite{scaffolding}. 
					\end{itemize}
				
				\end{itemize}
	
			\end{itemize}
	
		\end{frame}
	

%-------------------------------------------------------------------------------
	\section{Remediation}
%-------------------------------------------------------------------------------
	
		\subsection*{}

		\begin{frame}			
		
			\frametitle{How to Fix Discrimination in ML?}
			\Large \noindent 
			\textbf{Fix organizational processes}: Lecture 6\\
			\noindent 
			\textbf{Fix the data}:
			\begin{itemize}
				\item Collect demographically representative training data.
				\item Label and annotate data carefully.
				\item Select features judiciously.
				\item Sample and reweigh training data to minimize discrimination.\cite{kamiran2012data}
			\end{itemize}
			
		\end{frame}	
		
			
		\begin{frame}			
		
			\frametitle{How to Fix Discrimination in ML?}			
			
			\noindent 
			\textbf{Fix the model}:
			\begin{itemize}
				\item Consider fairness metrics when selecting hyperparameters and cutoff thresholds.
				\item Train fair models directly:
				\begin{itemize}
					\item Learning fair representations (LFR) and adversarial de-biasing.\cite{zemel2013learning}, \cite{zhang2018mitigating}
					\item Use dual objective functions that consider both accuracy and fairness metrics.
				\end{itemize}
				\item Edit model mechanisms to ensure less biased predictions, e.g., with \href{https://github.com/interpretml/interpret}{GA2M/EBM} models.
			\end{itemize}		
			\noindent \textbf{Fix the predictions}: 
			\begin{itemize}
				\item Balance model predictions, e.g., reject-option classification.\cite{kamiran2012decision}	
				\item Correct or override predictions with model assertions or appeal mechansims.\cite{hall2019guidelines}, \cite{kangdebugging}
			\end{itemize}
			
		\end{frame}
	
		\begin{frame}
	
			\frametitle{How to Fix Discrimination in ML?}		
			\centering
			Consider discrimination measures during model selection.\\
			\vspace{10pt}
			{\includegraphics[scale=0.09]{../img/lecture_3.png}}
			
		\end{frame}	
		
		\begin{frame}
		
			\frametitle{How to Fix Discrimination in ML?}		
			\centering
			As part of a responsible ML workflow.\\
			\vspace{10pt}
			{\includegraphics[scale=0.08]{../img/rml_diagram_no_hilite.png}}
			
		\end{frame}		
		
%-------------------------------------------------------------------------------
	\section{Acknowledgements}
%-------------------------------------------------------------------------------

	\subsection*{}
	
	\begin{frame}
	
		\frametitle{Acknowledgements}
		
		This presentation borrows heavily from the expertise of Nicholas Schmidt and Bryce Stephens of \href{https://www.bldsllc.com/}{BLDS, LLC}, a leading fair lending compliance firm.\\
		\vspace{10pt}		
		Thanks to Lisa Song for her continued assistance in developing these course materials.\\
		\vspace{10pt}
		Some materials \copyright\hspace{1pt}Patrick Hall and the H2O.ai team 2017-2020.  

	\end{frame}	
	
%-------------------------------------------------------------------------------
%	References
%-------------------------------------------------------------------------------

	\begin{frame}[t, allowframebreaks]
	
		\frametitle{References}
		\printbibliography
		
	\end{frame}

\end{document}